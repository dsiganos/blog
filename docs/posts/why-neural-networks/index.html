<!DOCTYPE html>
<html lang="en-gb">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Why Neural Networks | Dimitrios Siganos&#39; Blog</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="Why neural networks? (an article I wrote back in 1996) This is an article I wrote about neural networks, back in 1996, as part of a university project. The article was lost but I found it on the wayback machine and I am republishing it since its content is still applicable today and the article was very popular in its day.
For the last ten years Neural networks have attracted a great deal of attention.">
    <meta name="generator" content="Hugo 0.121.1">
    
    
    
    
      <meta name="robots" content="noindex, nofollow">
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    

    
      

    

    

    
      <link rel="canonical" href="http://blog.siganos.org/posts/why-neural-networks/">
    

    <meta property="og:title" content="Why Neural Networks" />
<meta property="og:description" content="Why neural networks? (an article I wrote back in 1996) This is an article I wrote about neural networks, back in 1996, as part of a university project. The article was lost but I found it on the wayback machine and I am republishing it since its content is still applicable today and the article was very popular in its day.
For the last ten years Neural networks have attracted a great deal of attention." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://blog.siganos.org/posts/why-neural-networks/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-12-18T12:10:05+00:00" />
<meta property="article:modified_time" content="2023-12-18T12:10:05+00:00" />

<meta itemprop="name" content="Why Neural Networks">
<meta itemprop="description" content="Why neural networks? (an article I wrote back in 1996) This is an article I wrote about neural networks, back in 1996, as part of a university project. The article was lost but I found it on the wayback machine and I am republishing it since its content is still applicable today and the article was very popular in its day.
For the last ten years Neural networks have attracted a great deal of attention."><meta itemprop="datePublished" content="2023-12-18T12:10:05+00:00" />
<meta itemprop="dateModified" content="2023-12-18T12:10:05+00:00" />
<meta itemprop="wordCount" content="933">
<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Why Neural Networks"/>
<meta name="twitter:description" content="Why neural networks? (an article I wrote back in 1996) This is an article I wrote about neural networks, back in 1996, as part of a university project. The article was lost but I found it on the wayback machine and I am republishing it since its content is still applicable today and the article was very popular in its day.
For the last ten years Neural networks have attracted a great deal of attention."/>

	
  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        Dimitrios Siganos&#39; Blog
      
    </a>
    <div class="flex-l items-center">
      

      
      
<div class="ananke-socials">
  
</div>

    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked ttu">
          
        Posts
      </aside>
      










  <div id="sharing" class="mt3 ananke-socials">
    
  </div>


      <h1 class="f1 athelas mt3 mb1">Why Neural Networks</h1>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2023-12-18T12:10:05Z">December 18, 2023</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><h1 id="why-neural-networks">Why neural networks?</h1>
<h2 id="an-article-i-wrote-back-in-1996"><em>(an article I wrote back in 1996)</em></h2>
<p><em><strong>This is an article I wrote about neural networks, back in 1996, as part of a university project. The article was lost but I found it on the <a href="http://web.archive.org/web/20141127065609/http://www.doc.ic.ac.uk/~nd/surprise_96/journal/vol1/ds12/article1.html">wayback machine</a> and I am republishing it since its content is still applicable today and the article was very popular in its day.</strong></em></p>
<p>For the last ten years Neural networks have attracted a great deal of attention. They offer an alternative approach to computing and to understanding of the human brain. This approach is not something new. The first artificial neuron was produced in 1943 by the neurophysiologist Warren McCulloch and the logician Walter Pits. During the sixties, for reasons that are out of the scope of this article, people turned away from neural networks and concentrated in the symbolic side of Artificial Intelligence. Only in the eighties scientists saw the real potential of neural networks.</p>
<p>Neural networks take a different approach to problem solving than that of conventional computers. Conventional computers use an algorithmic approach i.e. the computer follows a set of instructions in order to solve a problem. Unless the specific steps that the computer needs to follow are known, the computer cannot solve the problem. That restricts the problem solving capability of conventional computers to problems that we already understand and know how to solve. But computers would be so much more useful, if they could do things that we don’t exactly know how to do.</p>
<p>Neural networks process information in a similar way the human brain does. The network is composed of a large number of highly interconnected processing elements(neurons) working in parallel to solve a specific problem. Neural networks learn by example. They cannot be programmed to perform a specific task. The examples must be selected carefully otherwise useful time is wasted or even worse the network might be functioning incorrectly. The problem is that there is no way of knowing if the system is faulty or not, unless an error occurs.</p>
<p>The building block of a neural net is the neuron. An artificial neuron works much the same way the biological one does. It takes many inputs having different weightings and has one output which depends on the inputs. A biological neuron can either ‘fire’ or not ‘fire’(When a neuron fires, it outputs a pulse signal of a few hundred Hz). In an artificial neuron ‘firing’ is normally represented by a logical one and nor ‘firing’ by a logical zero.</p>
<blockquote>
<p>“Definition: Neural computing is the study of networks of adaptable nodes which, through a process of learning from task examples, store experiential knowledge and make it available for use.” Aleksander, I. and Morton, H.</p>
</blockquote>
<p>Neural nets are widely used in pattern recognition because of their ability to generalise and to respond to unexpected inputs/patterns. During training, neurons are taught to recognise various specific patterns and whether to fire or not when that pattern is received. If a pattern is received during the execution stage that is not associated with an output, the neuron selects the output that corresponds to the pattern from the set of patterns that it has been taught of, that is least different from the input. This is called generalisation.</p>
<p>For example:
A 4-input neuron is trained to fire when the input is 1111 and not to fire when the input is 0000. After applying the generalisation rule the neuron will also fire when the input is 0111, 1011, 1101, 1110 or 1111 but will not fire when the input is 0000, 0001, 0010, 0100 or 1000. Any other inputs (like 0011) will produce a random output since they are equally distant from 0000 and 1111.</p>
<p>Highly complex pattern recognition can be achieved by using a network of neurons hence the name neural networks. The networks normally used for pattern recognition are called feed forward because they have no feedback. They simply associate inputs with outputs. Neural networks are now used successfully in speech, language, and image recognition. Of course, pattern recognition can be done successfully on conventional computers too, but writing the software is time consuming and the system will generally have a slower response.</p>
<p>Pattern reconstruction is much more complicated and something that on conventional computers is very difficult to do. For pattern reconstruction feed-forward networks are not enough. Feedback is needed in order to create a dynamic system that will produce the appropriate pattern. The output of each neuron is connected to the input of the neighbouring neurons. These kind of networks are called autoassociative networks.</p>
<p>An interesting experiment was carried out involving a neural network controlling a vehicle. The experiment was intended to compare human driving behaviour with neural network driving behaviour. The results showed an astonishing similarity between the two of them (Ref. 2). According to the results neural nets can approximate human driving behaviour with a maximum error of 5%.</p>
<p>Neural networks and conventional algorithmic computers are not in competition but complement each other. There are tasks that are more suited to an algorithmic approach like arithmetic operations and tasks that are more suited to neural networks. Even more, a large number of tasks require systems that use a combination of the two approaches (normally a conventional computer is used to supervise the neural network) in order to perform at maximum efficiency.</p>
<blockquote>
<p><em>Neural networks do not perform miracles. But if used sensibly they can produce some amazing results.</em></p>
</blockquote>
<p>References:</p>
<ol>
<li>An introduction to neural computing. Aleksander, I. and Morton, H. 2nd edition</li>
<li>Developments in autonomous vehicle navigation. Stefan Neuber, Jos Nijhuis, Lambert Spaanenburg. Institut fur Mikroelektronik Stuttgart, Allmandring 30A, 7000 Stuttgart-80</li>
</ol>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://blog.siganos.org/" >
    &copy;  Dimitrios Siganos' Blog 2023 
  </a>
    <div>
<div class="ananke-socials">
  
</div>
</div>
  </div>
</footer>

  </body>
</html>
